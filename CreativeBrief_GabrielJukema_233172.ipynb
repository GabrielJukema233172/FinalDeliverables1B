{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Block Y1B: Creative Brief Template\n",
    "\n",
    "Please, use this template to write down your solutions to the DataLab Tasks. If you have any questions, please, contact your mentor or the content responsible. \n",
    "\n",
    "## Important Notes:\n",
    "- [ ] Please, rename the file to ```CreativeBrief_<your_name>_<studentnumber>.ipynb``` before submitting it. \n",
    "- [ ] Upload this template to the 'Deliverables' folder in your BUas GitHub repository.\n",
    "- [ ] You are allowed to add as many (Markdown/Python) cells as you need. \n",
    "- [ ] If the task requires you to only write code or text, please, delete the unnecessary cell.\n",
    "- [ ] Your work should be reproducible, meaning that we should be able to run your code in the template and get the same results as you did. Tip: use relative paths to load your data!\n",
    "- [ ] Ensure that before you hand in the template, you press ```Restart & Run all```; we should be able to see the results of your code in the notebook (i.e., output cells).\n",
    "- [ ] Ensure that your code in the template is ```error-free```. In other words, we should not see any error messages when we run your code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Overview\n",
    "This project focuses on the analysis of diabetes patient data in order to apply in machine learning algorithms. <br>\n",
    "It will involve the students working on a wide variety of machine learning techniques, ranging from basic data analysis to the optimisation of advanced models. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Task 1: Exploratory Data Analysis (EDA) with Python and SQL\n",
    "_________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **Task 1A: Exploratory Data Analysis with SQL**\n",
    "\n",
    "### Task Description\n",
    "Etablish a connection between Python and a database with performing basic operations using SQL and perform exploratory data analysis with sql to understand the dataset's characteristics, patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1A.1: General Overview of the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the total number of records in the encounter table\n",
    "  \n",
    "    # Your code here.\n",
    "\n",
    "# Check the distribution of different admission types\n",
    "\n",
    "    # Your code here.\n",
    "  \n",
    "# Explore the top discharge dispositions\n",
    "\n",
    "    # Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1A.2: Identifying Missing or Anomalous Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for missing values in the race column (Hint: Count the occurance of each unique value in this column)\n",
    "\n",
    "    # Your code here\n",
    "\n",
    "#Check for missing or unusual values in the weight column\n",
    "\n",
    "    #Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1A.3: Understanding Age Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the age distribution of the patients\n",
    "\n",
    "    #Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1A.4: Admission Trends by Source and Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze how different admission sources contribute to hospital admissions \n",
    "# (Hint: Calculate the number of admissions per source)\n",
    "\n",
    "    #Your code here\n",
    "\n",
    "#Investigate which admission types correspond to specific admission sources\n",
    "# (Hint: add admission type to your previous query)\n",
    "\n",
    "    #Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1A.5: Hospital Stay and Readmission Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the average time in hospital for each admission type\n",
    "\n",
    "    #Your code here\n",
    "\n",
    "#Investigate readmission rates by admission type\n",
    "\n",
    "    #Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1A.6: Comparing Admission Types and Outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare discharge dispositions across different admission types\n",
    "\n",
    "    #Your code here\n",
    "\n",
    "# Compare readmission rates by discharge disposition\n",
    "\n",
    "    #Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+++++\n",
    "### **Task 1B: Exploratory Data Analysis with Python**\n",
    "\n",
    "### Task Description\n",
    "Perform comprehensive exploratory data analysis to understand the dataset's characteristics, patterns, and potential challenges. <br>\n",
    "It is crucial to understand the structure and quality of the dataset before diving into any analysis or modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1B.1: Loading the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import\n",
    "\n",
    "# Load the dataset\n",
    "\n",
    "# Display first few rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1B.2: Analysing the dataset shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset shape\n",
    "\n",
    "# Column names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1B.3: Load and Explore a Dataset Using NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "data = np.genfromtxt()\n",
    "\n",
    "# Check the shape of the dataset\n",
    "print(\"Dataset shape:\",)\n",
    "\n",
    "# Preview the first few rows\n",
    "print(\"First 5 rows of the dataset:\\n\",)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1B.4: Analysing data types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data types\n",
    "print(\"Data types:)\n",
    "\n",
    "# Unique values in categorical columns\n",
    "\n",
    "    print(f\"Unique values in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1B.5: Exploratory Data Analysis (EDA) with Visualisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import seaborn \n",
    "\n",
    "# Distribution of a numeric column\n",
    "\n",
    "\n",
    "# Count plot of a categorical variable\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Boxplot to detect outliers\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________\n",
    "## *Task 2: Data Processing\n",
    "### Task Description\n",
    "Create a robust data preprocessing pipeline to handle missing values, encode categorical variables, and scale numerical features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Task 2.1: Initial Cleaning and Pre-processing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1: Load and Visualise the Data\n",
    "- Use pandas to load the dataset.\n",
    "- Visualise the first few rows to understand the structure of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required Python libraries\n",
    "import pandas as pd\n",
    "\n",
    "# Loading the data\n",
    "\n",
    "  #<add your code here>\n",
    "\n",
    "# visualise the first few rows of the data\n",
    "\n",
    "  #<add your code here>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2: Display Data Types and Check for Incorrect Types\n",
    "\n",
    "- Use the .dtypes attribute to display each column's data type.\n",
    "- Identify columns that have unexpected data types (e.g., numeric data stored as strings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display data types of each column\n",
    "\n",
    "   #<add your code here>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.3: Identify and Remove Duplicate Rows\n",
    "\n",
    "- Check for duplicate rows.\n",
    "- If duplicates exist, remove them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate rows\n",
    " #<add your code here>\n",
    "\n",
    "print(f\"Number of duplicate rows: {}\")\n",
    "\n",
    "# Drop duplicates if any\n",
    "   #<add your code here>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2: Handling Metadata and Missing Values\n",
    "#### 2.2.1: Identify Metadata from the Dataset\n",
    "Metadata includes information like column names, data types, and any additional descriptive information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display metadata including column names and data types\n",
    " #<add your code here>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 2.2.2: Analyse Different Types of Data\n",
    "- Categorize columns based on their data types (e.g., numerical, categorical).\n",
    "- Describe the significance of each data type and how it might impact data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe data to understand different types\n",
    " #<add your code here>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3: Identify Missing Values\n",
    "- Missing values can significantly impact model performance and analysis.\n",
    "- Count missing values in each column.\n",
    "- Calculate the percentage of missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify and calculate the percentage of missing values\n",
    " #<add your code here>\n",
    "print(\"Missing values (in percentage) for each column:\\n\",  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.4: Splitting Data for Analysis\n",
    "Splitting data ensures that models are trained on one portion of the data and tested on another, unseen portion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Features and target\n",
    "X = df.drop(columns=['target_column_name'])\n",
    "y = df['target_column_name']\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________\n",
    "## *Task 3: Machine Learning\n",
    "### Task Description\n",
    "Implement various machine learning algorithms for regression, classification and clustering and evaluating them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Task 3.1: Implementing Regression Baseline**\n",
    "#### 3.1.1: Loading the data\n",
    "- Use pandas to load the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2: Start to implement Linear Regression as a baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing requires sklearn packages for implementing linear regression\n",
    "from sklearn.model_selection import \n",
    "\n",
    "# Defining features (X) and target (y)\n",
    "\n",
    "\n",
    "# Spliting data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split()\n",
    "\n",
    "# Initialising and training Linear Regression model\n",
    "linear_model = LinearRegression()\n",
    "\n",
    "\n",
    "# Predictions\n",
    "y_pred = linear_model.predict()\n",
    "\n",
    "# Evaluation\n",
    "mae \n",
    "mse \n",
    "\n",
    "print(f\"Linear Regression MAE: {}\")\n",
    "print(f\"Linear Regression MSE: {}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### You could try other linear models as well, such as Ridge, Lasso, useful information can be found [here](https://scikit-learn.org/stable/modules/linear_model.html#linear-model)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.3: Visualise the output for linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(X.shape[1]):\n",
    "    plt.scatter()\n",
    "    plt.xlabel()\n",
    "    plt.ylabel()\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.4: Implement Decision Tree Regression for Non-linear Relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing requires sklearn packages for implementing regression\n",
    "from sklearn.model_selection import \n",
    "\n",
    "# Initialising and training Decision Tree Regressor\n",
    "tree_model = DecisionTreeRegressor()  \n",
    "\n",
    "\n",
    "# Predictions\n",
    "y_pred_tree = tree_model.predict()\n",
    "\n",
    "# Evaluation\n",
    "\n",
    "\n",
    "print(f\"Decision Tree Regression MAE: {}\")\n",
    "print(f\"Decision Tree Regression MSE: {}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.5: Implement Gradient Boosting Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing requires sklearn packages for implementing Gradient Boosting Regressor\n",
    "from sklearn.ensemble import\n",
    "\n",
    "# Initialising and training Gradient Boosting Regressor\n",
    "gboost_model = GradientBoostingRegressor()\n",
    "\n",
    "\n",
    "# Predictions\n",
    "y_pred_gboost = gboost_model.predict()\n",
    "\n",
    "# Evaluation\n",
    "mae_gboost = mean_absolute_error()\n",
    "mse_gboost = mean_squared_error()\n",
    "\n",
    "print(f\"Gradient Boosting Regression MAE: {}\")\n",
    "print(f\"Gradient Boosting Regression MSE: {}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.6: Implement XGBoost Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing requires packages\n",
    "\n",
    "\n",
    "# Initialize and train XGBoost Regressor\n",
    "xgboost_model = xgb.XGBRegressor()\n",
    "xgboost_model.fit()\n",
    "\n",
    "# Predictions\n",
    "y_pred_xgboost = xgboost_model\n",
    "\n",
    "# Evaluation\n",
    "mae_xgboost = mean_absolute_error()\n",
    "mse_xgboost = mean_squared_error()\n",
    "\n",
    "print(f\"XGBoost Regression MAE: {}\")\n",
    "print(f\"XGBoost Regression MSE: {}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.7: Visualise the output for non-linear regression\n",
    "Try to plot histogram of features, useful information can be found [here](https://matplotlib.org/stable/gallery/statistics/hist.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write visualisation code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.8: Evaluation and Model Comparison\n",
    "compare the performance of all four models: Linear Regression, Decision Tree Regression, Gradient Boosting, and XGBoost. Summarise each model’s performance using a comparison table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample comparison table\n",
    "results = {\n",
    "    \"Model\": [\"Linear Regression\", \"Decision Tree\", \"Gradient Boosting\", \"XGBoost\"],\n",
    "    \"MAE\": [],\n",
    "    \"MSE\": []\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame()\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expected Output for model comparison in 3.1.8:\n",
    "\n",
    "| Model | MAE | MSE |\n",
    "|---|---|---|\n",
    "| Linear Regression | -- | -- |\n",
    "| Decision Tree     | -- | -- |\n",
    "| Gradient Boosting | -- | -- |\n",
    "| XGBoost           | -- | -- |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "++++++++\n",
    "### **3.2: Implementing Classification Baseline**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1: Loading the data\n",
    "- Use pandas to load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2:  Prepare Data for Classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing requires sklearn packages\n",
    "from sklearn.model_selection import \n",
    "\n",
    "# Defining target and features\n",
    "X = df.drop()  \n",
    "\n",
    "\n",
    "# Converting target to binary if needed\n",
    "\n",
    "\n",
    "# Spliting data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.3:  Implement Logistic Regression as a Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing requires sklearn packages\n",
    "from sklearn.linear_model import \n",
    "from sklearn.metrics import \n",
    "\n",
    "# Initialising and training Logistic Regression model\n",
    "logistic_model = \n",
    "\n",
    "\n",
    "# Predictions\n",
    "\n",
    "\n",
    "# Evaluation\n",
    "accuracy = \n",
    "\n",
    "\n",
    "print(f\"Logistic Regression Accuracy: {}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.4: Implement Random Forest Classifier as a Non-linear Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing requires sklearn packages\n",
    "from sklearn.ensemble import \n",
    "\n",
    "# Initialising and train Random Forest Classifier\n",
    "\n",
    "\n",
    "# Predictions\n",
    "\n",
    "\n",
    "# Evaluation\n",
    "accuracy =\n",
    "\n",
    "print(f\"Random Forest Accuracy: {}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.5: Implement K-Nearest Neighbors (KNN) Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing requires sklearn packages\n",
    "from sklearn.neighbors\n",
    "\n",
    "# Standardising the features for KNN\n",
    "\n",
    "\n",
    "# Initialising and training the KNN model\n",
    "\n",
    "\n",
    "# Predictions\n",
    "y_pred_knn = \n",
    "\n",
    "# Evaluation\n",
    "accuracy = \n",
    "\n",
    "print(f\"KNN Accuracy: {}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.6: Evaluation and Model Comparison\n",
    "Compare the performance of implemented models using four metrics including \"Accuracy\", \"Precision\", \"Recall\", and \"F1-score\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\n",
    "    \"Model\": [\"Logistic Regression\", \"Random Forest\", \"K-Nearest Neighbors\"],\n",
    "    \"Accuracy\":\n",
    "}\n",
    "\n",
    "results_df = \n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expected Output for Model Comparison in 3.2.6:\n",
    "| Model | Accuracy | Precision | Recall | F1-score |\n",
    "|---|---|---|---|---|\n",
    "| Logistic Regression | -- | -- | -- | -- |\n",
    "| Random Forest       | -- | -- | -- | -- |\n",
    "| K-Nearest Neighbors | -- | -- | -- | -- |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "++++++++\n",
    "### **Task 3.3: Implementing Clustering Baseline**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.1: Load and Preprocess the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "\n",
    "# Standardising column names and remove duplicates\n",
    "\n",
    "\n",
    "# Applying standard scaling to numerical features for clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.2: Implement K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster\n",
    "\n",
    "# Initialising and training K-Means model\n",
    "kmeans = \n",
    "\n",
    "# Predict clusters and evaluate\n",
    "\n",
    "\n",
    "print(f\"K-Means Silhouette Score: {}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.3: Apply PCA for Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition \n",
    "\n",
    "# Applying PCA to reduce dimensions\n",
    "pca = \n",
    "\n",
    "# Visualising clusters for K-Means in 2D\n",
    "# Hint: Set n_components=2 for 2D visualisation or n_components=3 for 3D visualisation.\n",
    "plt.scatter()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.4: Implement Random Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster\n",
    "\n",
    "# Initialising and fiting hierarchical clustering\n",
    "hierarchical = \n",
    "\n",
    "# Evaluating with Silhouette Score\n",
    "hierarchical_silhouette = \n",
    "print(f\"Hierarchical Clustering Silhouette Score: {}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.5: Implement DBSCAN for Density-Based Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster\n",
    "\n",
    "# Initialising and fiting DBSCAN\n",
    "dbscan = \n",
    "\n",
    "# Filtering noise points (labeled as -1)\n",
    "\n",
    "\n",
    "# Evaluate with Silhouette Score (excluding noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.6: Evaluation and Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of Silhouette Scores\n",
    "results = {\n",
    "    \"Clustering Method\": [\"K-Means\", \"Hierarchical\", \"DBSCAN\"],\n",
    "    \"Silhouette Score\": []\n",
    "}\n",
    "\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expected Output for Model Comparison in 3.3.6:\n",
    "| Clustering Method | Silhouette Score |\n",
    "|     ---           |    --   |\n",
    "| K-Means           |  |\n",
    "| Hierarchical      |  |\n",
    "| DBSCAN            |  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "++++++++++\n",
    "### **3.4 Feature Engineering and Feature Selection**\n",
    "Select the most relevant features to improve model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.1: Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create age group categories if applicable\n",
    "X['age_group'] = \n",
    "\n",
    "# Interaction feature example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.2: Generate Statistical Features\n",
    "Calculate aggregate statistics such as mean, max, min, or standard deviation for features if relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "# Standardize numerical columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.3: Feature Transformation\n",
    "Normalise or standardise features, especially for distance-based models like KNN or clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "# Standardize numerical columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.4: Feature Selection\n",
    "Use feature importance from tree-based models (Random Forest, Gradient Boosting) to select important features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Fit Random Forest model\n",
    "\n",
    "\n",
    "# Get feature importance\n",
    "importances = model.feature_importances_\n",
    "\n",
    "\n",
    "# Plot feature importance\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.5: Model Evaluation After Feature Selection\n",
    "- After feature engineering and selection, evaluate your model with the new set of features.\n",
    "- Apply cross-validation to measure the model’s performance with the refined feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Fit model on selected features\n",
    "\n",
    "print(\"Cross-validated accuracy after feature selection:\",)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+++++++++\n",
    "### **Task 3.5: Hyperparameter Tuning**\n",
    "Improve the quality of your implementations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5.1: GridSearchCV for Systematic Hyperparameter Tuning\n",
    "- Define Hyperparameter Grid for Different Models\n",
    "- Define grids of hyperparameters for models such as Logistic Regression (for classification) and K-Means (for clustering)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Define hyperparameter grid for Logistic Regression\n",
    "\n",
    "\n",
    "# Define hyperparameter grid for KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5.2 Apply GridSearchCV with Cross-Validation\n",
    "- Use GridSearchCV with cross-validation to perform hyperparameter tuning on both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression GridSearchCV\n",
    "logistic_search = GridSearchCV(LogisticRegression(), logistic_grid, scoring='accuracy')\n",
    "logistic_search.fit(X_train, y_train)\n",
    "\n",
    "# KMeans GridSearchCV\n",
    "kmeans_search = GridSearchCV(KMeans(), kmeans_grid, scoring='silhouette_score')\n",
    "kmeans_search.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5.3: Nested Cross-Validation to Validate Stability of Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Logistic Regression Nested Cross-Validation\n",
    "nested_cv_score_logistic = cross_val_score()\n",
    "print(\"Nested CV Accuracy for Logistic Regression:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5.4: Evaluate Tuned Models with Multiple Metrics\n",
    "- Evaluate the tuned Logistic Regression model on both training and test sets.\n",
    "- Use multiple metrics for classification (accuracy, F1-score) and clustering (silhouette score, Calinski-Harabasz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, silhouette_score, calinski_harabasz_score\n",
    "\n",
    "# Logistic Regression evaluation\n",
    "y_train_pred = logistic_search.best_estimator_.predict(X_train)\n",
    "y_test_pred = logistic_search.best_estimator_.predict(X_test)\n",
    "\n",
    "train_accuracy = accuracy_score()\n",
    "test_accuracy = accuracy_score()\n",
    "train_f1 = f1_score()\n",
    "test_f1 = f1_score()\n",
    "\n",
    "print(f\"Logistic Regression - Train Accuracy: \")\n",
    "print(f\"Logistic Regression - Train F1: \")\n",
    "\n",
    "# KMeans evaluation\n",
    "kmeans_labels = kmeans_search.best_estimator_.predict(X_train)\n",
    "silhouette = silhouette_score()\n",
    "calinski_harabasz = calinski_harabasz_score()\n",
    "\n",
    "print(f\"KMeans - Silhouette Score: \")\n",
    "print(f\"KMeans - Calinski-Harabasz Score:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+++++++++\n",
    "### **Task 3.6: Model Selection**\n",
    "- Compare and evaluate the implemented models and select the best ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6.1: Compare and Choose the Best Model\n",
    "- Compare the models using evaluation metrics (e.g., MSE for regression, F1-score for classification, Silhouette Score for clustering).\n",
    "- Summarise the performance metrics of each model in a table and choose the model with the best performance for each task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample model comparison\n",
    "results = {\n",
    "    \"Model\": [\"Linear Regression\", \"Random Forest\", \"KMeans\"],\n",
    "    \"MAE\": [mae_lr, mae_rf, \"N/A\"],\n",
    "    \"MSE\": [mse_lr, mse_rf, \"N/A\"],\n",
    "    \"Silhouette Score\": [\"N/A\", \"N/A\", silhouette_score_kmeans]\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6.2: Finalise and Save the Best Model\n",
    "- Once the best model is identified, retrain it on the full dataset and save it using joblib for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the best model\n",
    "joblib.dump(grid_search.best_estimator_, \"best_model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____________\n",
    "## *Task 4: Mathematics for Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1: Implementing Linear Regression with Numpy from Scratch\n",
    "\n",
    "- Add all links to your Jupyter notebook files `LinearRegression.ipynb` here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2: Implement Logistic Regression with Numpy from Scratch\n",
    "\n",
    "- Add all links to your Jupyter notebook files `LogisticRegression.ipynb` here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____________\n",
    "## *Task 5: Final Report for Your Creative Brief Project\n",
    "- Add a link to your `FinalReport_<your_name>_<studentnumber>.pdf` here!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
